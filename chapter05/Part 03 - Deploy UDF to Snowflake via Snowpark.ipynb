{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b79eb654",
   "metadata": {},
   "source": [
    "# Chapter 5 - Part 3: Deploy UDF to Snowflake via Snowpark\n",
    "\n",
    "Welcome to chapter 5 of our Snowflake Data Scientist training series.\n",
    "\n",
    "In chapter 5 we will look at model deployment options. The code is structured into three parts:\n",
    "- Part 1: We train a model and save its binary file to disk, called the \"pickle file\".\n",
    "- Part 2: We deploy the pickle file to an Azure Function and call it via API Integration from Snowflake\n",
    "- Part 3: We deploy the pickle file directly to Snowflake using SnowPark.\n",
    "\n",
    "Happy coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18af78f1",
   "metadata": {},
   "source": [
    "## ATTENTION: Python 3.8 is needed to run this notebook.\n",
    "It's the only Python version that Snowpark currently supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05a2a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make sure you have Python 3.8 installed on your system for this. Else the package won't install.\n",
    "!pip install snowflake-snowpark-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5242b55b",
   "metadata": {},
   "source": [
    "### 1.) Connecting to Snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29072e4a",
   "metadata": {},
   "source": [
    "To connect to your Snowflake instance, make sure you have all requirements installed and your connection details ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae241b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, sys\n",
    "assert sys.version_info == (3, 8), \"You really need to run Python 3.8 for this code.\"\n",
    "import snowflake.snowpark as snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b2921",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('state.json') as sdf:\n",
    "    state_dict = json.load(sdf)    \n",
    "\n",
    "session = snp.Session.builder.configs(state_dict[\"connection_parameters\"]).create()\n",
    "session.use_warehouse(state_dict['compute_parameters']['default_warehouse'])\n",
    "return session, state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcd25b3",
   "metadata": {},
   "source": [
    "### 2.) Start Working with simple example\n",
    "As a first step we will work on a simple example. Let's multiply some integers and see how it all comes together.\n",
    "\n",
    "The steps that need to be done:\n",
    "- define the UDF in Python as you normally would\n",
    "- register the UDF in Snowflake via Snowpark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48ec51d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_by_four(input_int_py: int):\n",
    "  return input_int_py*4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f362b48b",
   "metadata": {},
   "source": [
    "Next up we register UDF in Snowflake. Also have a look at interworks code repository and their great examples: https://github.com/interworks/Snowflake-Python-Functionality/tree/main/Created%20via%20Snowpark/User%20Defined%20Functions.\n",
    "\n",
    "Make sure you have an internal stage \"UDF_STAGE\" available in Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ecf46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add packages and data types\n",
    "from snowflake.snowpark.types import IntegerType\n",
    "\n",
    "### Create the temp stage\n",
    "session.sql('create stage if not exists UDF_STAGE').collect() \n",
    "\n",
    "### Upload UDF to Snowflake\n",
    "session.udf.register(\n",
    "    func = multiply_by_four\n",
    "  , return_type = IntegerType()\n",
    "  , input_types = [IntegerType()]\n",
    "  , is_permanent = True\n",
    "  , name = 'snp_multiply_by_four'\n",
    "  , replace = True\n",
    "  , stage_location = '@UDF_STAGE'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e220e2b",
   "metadata": {},
   "source": [
    "Finally lets do some testing, first via Snowpark, later via Snowflake UI directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a977f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql('''\n",
    "  SELECT snp_multiply_by_four(20)\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34fa9f1",
   "metadata": {},
   "source": [
    "Done! We now have a working UDF using Snowpark in Snowflake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3793244",
   "metadata": {},
   "source": [
    "### 3.) Uploading our real UDF to Snowflake\n",
    "Again we have to do two steps:\n",
    "- define the UDF in Python as you normally would\n",
    "- register the UDF in Snowflake via Snowpark\n",
    "\n",
    "Our UDF in this case is just the inference function of our model. We load the pickle file and then call predict on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6cbea10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import sys\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = joblib.load('randomforest_classifier.joblib.pkl')\n",
    "\n",
    "def predict_bp_after(sex_agrgrp_position, bp_before):       \n",
    "    return rfr.predict([[sex_agrgrp_position, bp_before]])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e2229c",
   "metadata": {},
   "source": [
    "Next up we register UDF in Snowflake. Again make sure you have an internal stage \"UDF_STAGE\" available in Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f19391",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add packages and data types\n",
    "from snowflake.snowpark.types import StringType, IntegerType, FloatType\n",
    "session.add_packages(['pandas', 'scikit-learn', 'joblib'])\n",
    "session.add_import('randomforest_classifier.joblib.pkl')\n",
    "\n",
    "### Create the temp stage\n",
    "session.sql('create stage if not exists UDF_STAGE').collect() \n",
    "\n",
    "### Upload UDF to Snowflake\n",
    "session.udf.register(\n",
    "    func = predict_bp_after\n",
    "  , return_type = FloatType()\n",
    "  , input_types = [IntegerType(), IntegerType()]\n",
    "  , is_permanent = True\n",
    "  , name = 'snp_predict_bp_after'\n",
    "  , replace = True\n",
    "  , stage_location = '@UDF_STAGE'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b8c7f2",
   "metadata": {},
   "source": [
    "Finally lets do some testing, first via Snowpark, later via Snowflake UI directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd3ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql('''\n",
    "  SELECT snp_predict_bp_after(0, 155)\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38929032",
   "metadata": {},
   "source": [
    "### Done. Close it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47703d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
